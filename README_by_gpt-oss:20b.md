# ğŸŒŸ StoryCode & Codeâ€‘Mate: A Multiâ€‘Purpose Flask API
> **An allâ€‘inâ€‘one toolkit that turns code into stories, art, jokes, and much more**
> Powered by an Ollama LLM (default: `gptâ€‘oss:20b`)

---

## Table of Contents

| Section | Link |
|---------|------|
| ğŸ¯ Overview | #overview |
| ğŸš€ Quick Start | #quick-start |
| ğŸ“¦ Installation | #installation |
| âš™ï¸ Configuration | #configuration |
| ğŸ“– Endpoints & Features | #endpoints |
| ğŸ§ª Testing | #testing |
| ğŸ“ Documentation | #documentation |
| ğŸ› ï¸ Contributing | #contributing |
| ğŸ“„ License | #license |
| ğŸ™ Acknowledgements | #acknowledgements |

---

## ğŸ¯ Overview

This Flask app bundles **nine distinct creativeâ€‘coding helpers** in a single web service:

| Feature | What it does |
|---------|--------------|
| **StoryCode** | Turns Python code snippets into whimsical fairyâ€‘tales |
| **Mentalâ€‘Health Chatbot** | Empathetic support for stressed developers |
| **Artâ€‘Generator** | Generates SVG art from a programming challenge |
| **Jokeâ€‘Generator** | Oneâ€‘liner codeâ€‘bug jokes + explanation |
| **Commitâ€‘Poet** | Write Git commit messages in a chosen style |
| **Regexâ€‘Wizard** | Generate or explain regex patterns |
| **Errorâ€‘Sleuth** | Debug error messages & stack traces |
| **APIâ€‘Mockup** | Produce realistic JSON mockâ€‘data for a data model |
| **Docâ€‘Writer** | Generate Googleâ€‘style docstrings for a Python function |
| **Codeâ€‘Visualizer** | Mermaid.js flowchart from Python code |

All routes are **RESTâ€‘style** and render simple, selfâ€‘contained HTML pages.
Behind the scenes they all call the Ollama LLM (via the `/api/generate` endpoint) with a carefully crafted prompt.

> **Why use Ollama?**
> - Runs locally, no API key required.
> - Free & fast.
> - Works with any model you have downloaded (`gptâ€‘oss:20b` is shipped with the repo as a sane default).

---

## ğŸš€ Quick Start

> **Prerequisites** â€“ Python 3.10+, `pip`, and a running Ollama server.

```bash
# 1ï¸âƒ£ Clone
git clone https://github.com/your-username/storycode.git
cd storycode

# 2ï¸âƒ£ (Optional) Create a virtual environment
python -m venv .venv
source .venv/bin/activate   # Windows: .venv\Scripts\activate

# 3ï¸âƒ£ Install dependencies
pip install -r requirements.txt

# 4ï¸âƒ£ Download the default LLM (once)
ollama pull gpt-oss:20b

# 5ï¸âƒ£ Start the app
python app.py

# 6ï¸âƒ£ Open your browser
http://127.0.0.1:5000
```

You should see the home page with links to every feature.

---

## ğŸ“¦ Installation

### Clone the repo

```bash
git clone https://github.com/your-username/storycode.git
cd storycode
```

### Install dependencies

```bash
pip install -r requirements.txt
```

> `requirements.txt` contains:
> - Flask
> - requests
> - (Optional) jinja2â€‘excel if you want to export tables

### Set up Ollama

```bash
# Download and install the Ollama client if you havenâ€™t already
# https://ollama.ai/download

# Pull the default model (20â€¯GB, may take a while)
ollama pull gpt-oss:20b
```

> **Tip**: If you want a smaller model, replace `MODEL` in `app.py` with e.g. `mistral:7b`.

---

## âš™ï¸ Configuration

| Variable | Default | Description |
|----------|---------|-------------|
| `OLLAMA_URL` | `http://localhost:11434/api/generate` | Base URL for your Ollama server |
| `MODEL` | `gpt-oss:20b` | The LLM you want to use |
| `DEBUG` | `True` (in `app.run`) | Flask debug mode |

If you prefer environment variables:

```bash
export OLLAMA_URL="http://localhost:11434/api/generate"
export MODEL="mistral:7b"
```

The app will pick them up automatically via `os.getenv()` (extend `app.py` accordingly).

---

## ğŸ“– Endpoints & Features

Below is a quick reference to each route, the form fields it expects, and the prompt logic it uses.

| Route | Method | Form Fields | Example Request | What youâ€™ll see |
|-------|--------|-------------|-----------------|-----------------|
| `/` | GET | â€“ | Home page (link menu) | Links to every feature |
| `/storycode` | GET/POST | `code_snippet` | Submit a snippet â†’ fairyâ€‘tale |
| `/mental-health` | GET/POST | `message` | Ask for support â†’ empathetic response |
| `/art-generator` | GET/POST | `challenge` | Submit a programming challenge â†’ SVG art |
| `/joke-generator` | GET/POST | `bug_description` | Generate joke & explanation (JSON) |
| `/commit-poet` | GET/POST | `commit_message`, `style` | Choose a style (e.g. Conventional, Gitmoji,
Conventional) |
| `/regex-wizard` | GET/POST | `regex_type` (`generate`/`explain`), `pattern` | Generate or explain a regex |
| `/error-sleuth` | GET/POST | `error_message` | Analyse and explain an error |
| `/api-mockup` | GET/POST | `description` | Sample JSON array of 3 objects |
| `/doc-writer` | GET/POST | `function_code`, `style` | Generate Googleâ€‘style docstring |
| `/code-visualizer` | GET/POST | `code_to_visualize` | Mermaid flowchart (cleaned) |

All pages are **userâ€‘friendly** â€“ simply paste your snippet / description into the text area, press *Submit*, and
the result appears right below.

> **Mermaid diagrams** are returned *without* code fences. The app cleans up duplicate `flowchart TD` declarations
for you.
> **Joke responses** come wrapped in a JSON object so you can consume them programmatically if you need to.

---

## ğŸ§ª Testing

### Unit tests

We provide a small test suite that mocks the Ollama call and verifies that each endpoint renders the expected
sections.

```bash
pip install pytest
pytest
```

> **If you run into â€œtimeoutâ€ errors** â€“ make sure the `OLLAMA_URL` is reachable.
> Adjust `timeout` in `call_ollama()` (currently set to 120â€¯s).

### Manual testing

1. Use Postman / curl to hit each endpoint directly.
   ```bash
   curl -X POST http://127.0.0.1:5000/storycode \
        -d "code_snippet=def hello():\n    print('Hello!')"
   ```

2. Inspect the raw LLM output in the debug console to fineâ€‘tune prompts.

---

## ğŸ“ Documentation

The HTML templates live in `templates/` â€“ theyâ€™re intentionally minimal, but you can add styling or copyâ€‘paste
features as you wish.

**Prompt helpers**
All LLM calls are routed through the `call_ollama()` function:

```python
def call_ollama(prompt: str) -> str:
    payload = {"model": MODEL, "prompt": prompt}
    resp = requests.post(OLLAMA_URL, json=payload, timeout=120)
    resp.raise_for_status()
    return resp.json()["response"]
```

Feel free to tweak the prompts in `app.py` if you want different output styles.

---

## ğŸ› ï¸ Contributing

Weâ€™d love contributions! Please follow these steps:

1. **Fork** and **branch**
   ```bash
   git checkout -b feature/awesome-new-feature
   ```

2. **Create tests** for any new functionality.
3. **Follow PEPâ€‘8** guidelines â€“ `black` and `flake8` are preâ€‘configured.
4. Submit a **Pull Request** â€“ include a concise description of the change.

> **Note**: When adding a new endpoint, remember to update the README so the community knows how to use it.

---

## ğŸ“„ License

MIT Â© 2024 **Your Name**
See the [LICENSE](LICENSE) file for details.

---

## ğŸ™ Acknowledgements

| Resource | Purpose |
|----------|---------|
| **Ollama** â€“ https://ollama.ai | Local LLM inference |
| **Flask** â€“ https://flask.palletsprojects.com/ | Lightweight web framework |
| **Mermaid.js** â€“ https://mermaid-js.github.io/mermaid/ | Flowchart rendering |
| **OpenAI Cookbook** â€“ inspiration for prompt design | https://github.com/openai/openai-cookbook |
| **Google Docstring Style** â€“ for `doc-writer` | https://google.github.io/styleguide/pyguide.html |

Feel free to give us a star â­ if you found this helpful!

---
